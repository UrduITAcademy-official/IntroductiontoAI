{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot w/ Bard API"
      ],
      "metadata": {
        "id": "ib1AG4FukFEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment"
      ],
      "metadata": {
        "id": "OaGoxClLXoP6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5kK_XBJ1XHGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a57369-05a2-41a6-a92c-33e57d1ade41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bardapi\n",
            "  Downloading bardapi-0.1.38-py3-none-any.whl (39 kB)\n",
            "Collecting httpx[http2]>=0.20.0 (from bardapi)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bardapi) (2.31.0)\n",
            "Collecting deep-translator (from bardapi)\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama (from bardapi)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: google-cloud-translate in /usr/local/lib/python3.10/dist-packages (from bardapi) (3.11.3)\n",
            "Collecting browser-cookie3 (from bardapi)\n",
            "  Downloading browser_cookie3-0.19.1-py3-none-any.whl (14 kB)\n",
            "Collecting langdetect (from bardapi)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->bardapi) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->bardapi) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx[http2]>=0.20.0->bardapi)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->bardapi) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->bardapi) (1.3.0)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->bardapi)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx[http2]>=0.20.0->bardapi)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from browser-cookie3->bardapi)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from browser-cookie3->bardapi)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jeepney in /usr/lib/python3/dist-packages (from browser-cookie3->bardapi) (0.7.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator->bardapi) (4.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bardapi) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bardapi) (2.0.7)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-translate->bardapi) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-translate->bardapi) (2.3.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-translate->bardapi) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-translate->bardapi) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->bardapi) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator->bardapi) (2.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi) (1.62.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi) (2.17.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi) (1.60.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi) (1.48.2)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->bardapi)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->bardapi)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[http2]>=0.20.0->bardapi) (1.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-translate->bardapi) (0.5.1)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=b16a9daa05bf07cf3fde37da6735540d8bcc439d36fd38de33134aae0ad1e952\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: pycryptodomex, lz4, langdetect, hyperframe, hpack, h11, colorama, httpcore, h2, deep-translator, browser-cookie3, httpx, bardapi\n",
            "Successfully installed bardapi-0.1.38 browser-cookie3-0.19.1 colorama-0.4.6 deep-translator-1.11.4 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.2 httpx-0.26.0 hyperframe-6.0.1 langdetect-1.0.9 lz4-4.3.3 pycryptodomex-3.20.0\n"
          ]
        }
      ],
      "source": [
        "!pip install bardapi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bardapi import Bard\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "VF1KrU4urUJP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace XXXX with the values you get from __Secure-1PSID\n",
        "os.environ['_BARD_API_KEY']=\"fQgL7L_YRi0JA1pVcWsH4x0aYGsazB6WOI4sfw_3YxcvFSn2v_enG7CFngBjA_p3gojkTQ.\"\n",
        "#os.environ['_BARD_API_KEY']='xxxxx'"
      ],
      "metadata": {
        "id": "-rY42uSnXbOG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simplest Chatbot"
      ],
      "metadata": {
        "id": "SQW9eevUj8ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your input text\n",
        "input_text = \"Why is the sky blue?\"\n",
        "input_text = \"What is the current status of Ashes Series 2023\"\n",
        "input_text = \"When will be the general election in Pakistan?\"\n",
        "#input_text = \"What is the weather now in Lahore?\"\n",
        "print(Bard().get_answer(input_text)['content'])"
      ],
      "metadata": {
        "id": "DFnfJtYbXd_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31937de7-11e8-42ee-ec0e-095a60abf52c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elections are a complex topic with fast-changing information. To make sure you have the latest and most accurate information, try Google Search.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Chatbot"
      ],
      "metadata": {
        "id": "8NU1b3f5kdkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Summarizer"
      ],
      "metadata": {
        "id": "x0pzfK-gkzJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6krlXbIVwsZe",
        "outputId": "c44a81a6-7ca3-49a9-8f91-a9e8d3193757"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/278.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/278.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader"
      ],
      "metadata": {
        "id": "P6-zH35Dw0Xm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ptCfCdRprdOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750f2dc9-a5e8-4a23-ebbc-13749d572db7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#directory = '/content/drive/MyDrive/1-Build-a-Chatbot/'\n",
        "directory = '/content/drive/MyDrive/Personal_Family/UITA/Lecture6/1-Build-a-Chatbot/'\n",
        "filename  = 'Attention-Is-All-You-Need.pdf'\n",
        "#filename = 'UITA_AI_Outline.pdf'\n",
        "#filename  = 'Art-of-War.pdf'"
      ],
      "metadata": {
        "id": "n6d42reCXq7w"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a pdf file object\n",
        "pdfFileObject = open(directory+filename, 'rb')\n",
        "# creating a pdf reader object\n",
        "pdfReader = PdfReader(pdfFileObject)\n",
        "text=[]\n",
        "summary=' '\n",
        "#Storing the pages in a list\n",
        "for i in range(0,len(pdfReader.pages)):\n",
        "  # creating a page object\n",
        "  pageObj = pdfReader.pages[i].extract_text()\n",
        "  pageObj= pageObj.replace('\\t\\r','')\n",
        "  pageObj= pageObj.replace('\\xa0','')\n",
        "  # extracting text from page\n",
        "  text.append(pageObj)"
      ],
      "metadata": {
        "id": "gYpNlLqcZVyw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge multiple page - to reduce API Calls\n",
        "def join_elements(lst, chars_per_element):\n",
        "    new_lst = []\n",
        "    for i in range(0, len(lst), chars_per_element):\n",
        "        new_lst.append(''.join(lst[i:i+chars_per_element]))\n",
        "    return new_lst\n",
        "\n",
        "# Option to keep x elements per list element\n",
        "new_text = join_elements(text, 3)\n",
        "\n",
        "print(f\"Original Pages = \",len(text))\n",
        "print(f\"Compressed Pages = \",len(new_text))"
      ],
      "metadata": {
        "id": "0y8iM7trjKdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c409cc0c-c977-497a-e2e5-fd20fa639f11"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Pages =  15\n",
            "Compressed Pages =  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt):\n",
        "  response = Bard().get_answer(prompt)['content']\n",
        "  return response"
      ],
      "metadata": {
        "id": "AJ_lIoBzX267"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(new_text)):\n",
        "  prompt =f\"\"\"\n",
        "  Your task is to act as a Text Summariser.\n",
        "  I'll give you text from  pages of a book from beginning to end.\n",
        "  And your job is to summarise text from these pages in less than 100 words.\n",
        "  Don't be conversational. I need a plain 100 word answer.\n",
        "  Text is shared below, delimited with triple backticks:\n",
        "  ```{text[i]}```\n",
        "  \"\"\"\n",
        "  try:\n",
        "    response = get_completion(prompt)\n",
        "  except:\n",
        "    response = get_completion(prompt)\n",
        "  #print(response)\n",
        "  summary= summary+' ' +response +'\\n\\n'\n",
        "  #print(summary)\n",
        "  # result.append(response)\n",
        "  time.sleep(19)  #You can query the model only 3 times in a minute for free, so we need to put some delay\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "CxpOAcNuZV3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9022b180-e92e-45dd-83f9-604b1a8a3127"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  The paper proposes \"Transformer\", a novel neural network architecture for sequence transduction tasks like machine translation, replacing recurrent and convolutional layers with attention mechanisms solely. This achieves superior performance on WMT 2014 translation tasks, with 28.4 BLEU for English-to-German and 41.8 BLEU for English-to-French, significantly outperforming existing models. The Transformer also generalizes well to other tasks like English constituency parsing, demonstrating its versatility. Its parallelizability and faster training times make it a promising architecture for NLP. (99 words)\n",
            "\n",
            "\n",
            " The paper introduces the Transformer, a novel neural network architecture for sequence transduction tasks like machine translation and language modeling. Unlike previous models that rely on recurrent computation, the Transformer uses self-attention mechanisms to capture long-range dependencies between input and output elements. This enables significantly faster parallelization and achieves state-of-the-art translation quality. The Transformer architecture consists of an encoder-decoder structure, where the encoder maps an input sequence to a representation, and the decoder generates an output sequence one element at a time, both relying on self-attention and point-wise feed-forward layers. This approach avoids the sequential constraints of recurrent models and improves overall efficiency and performance.\n",
            "\n",
            "\n",
            "  **The Transformer model architecture consists of encoder and decoder stacks, each with six identical layers. Encoder layers have two sub-layers: multi-head self-attention and a feed-forward network. Decoder layers add a third sub-layer that performs multi-attention over encoder output. Residual connections and layer normalization are used around sub-layers. Attention maps queries and key-value pairs to outputs, where weights are computed by compatibility functions.**\n",
            "\n",
            "\n",
            "  **Scaled Dot-Product Attention** is a technique that calculates attention by dividing the dot products of queries and keys by the square root of the key dimension. This scaling improves performance for large key dimensions. **Multi-Head Attention** enhances attention by linearly projecting queries, keys, and values multiple times with different learned projections, performing attention on each projection, and concatenating the results. This allows the model to attend to information from different representation subspaces.\n",
            "\n",
            "\n",
            "  **Here is a 100-word summary of the text:**\n",
            "\n",
            "**Multi-head attention** is a technique that allows a model to attend to information from different representation subspaces at different positions. It is used in the Transformer model in three ways: encoder-decoder attention, encoder self-attention, and decoder self-attention. In addition to attention, the Transformer model also uses **position-wise feed-forward networks** and **embeddings**. To address the lack of recurrence or convolution, **positional encoding** is used to inject information about the relative or absolute position of tokens in the sequence.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(directory +'/bard_summary.txt',\n",
        "          'w') as out:\n",
        "  out.write(summary)"
      ],
      "metadata": {
        "id": "9SilyVEiX29Z"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}